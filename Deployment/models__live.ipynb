{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models__live.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX0I7R1SHFVQ",
        "outputId": "1b954e2e-3988-49d2-cf99-7e90f519e254"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WThzQN33cTdm"
      },
      "source": [
        "# Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIJnVx6PeSHE"
      },
      "source": [
        "# !pip install uvicorn\n",
        "# !pip install fastapi\n",
        "# !pip install nest_asyncio\n",
        "# !pip install pystan\n",
        "# !pip install prophet\n",
        "# !pip install pyngrok \n",
        "# !pip install pmdarima\n",
        "# !pip install -v scikit-learn==0.23.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IzDl7qxcqYC"
      },
      "source": [
        "# Importations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmOX3UNbh5at"
      },
      "source": [
        "# for FastAPI\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import pydantic\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "# for FBprophet\n",
        "from datetime import *\n",
        "import pandas_datareader as pdr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import holidays\n",
        "from prophet import Prophet\n",
        "# for arima\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import pmdarima as pm\n",
        "# for LSTM\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "# for transformer\n",
        "from utils.Time2Vector import Time2Vector\n",
        "from utils.Attention import MultiAttention, SingleAttention\n",
        "from utils.Encoder import TransformerEncoder\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48peMMDcTLYb"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas_datareader as pdr\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import pydantic\n",
        "import gunicorn\n",
        "import httptools\n",
        "from datetime import *\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model\n",
        "app = FastAPI()\n",
        "\n",
        "def lstm(data_set):\n",
        "  \"\"\"\n",
        "  Getting the desired data from yahoo, then doing some data manipulation such as data\n",
        "  reshaping\n",
        "  Args:\n",
        "      (str) data_set - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) diff_prediction - the model out-put (the prediction of the next day)\n",
        "      (float) real_prediction - the model output + today's price (real price of tomorrow)\n",
        "  \"\"\"\n",
        "\n",
        "  # data gathering\n",
        "  df = pdr.DataReader(data_set, data_source='yahoo', start=date.today() - timedelta(100))\n",
        "\n",
        "  # data manipulation\n",
        "\n",
        "  # creating a new df with Xt - Xt-1 values of the close prices (most recent 60 days)\n",
        "  close_df = df['2012-01-01':].reset_index()['Close'][-61:]\n",
        "  close_diff = close_df.diff().dropna()\n",
        "  data = np.array(close_diff).reshape(-1, 1)\n",
        "\n",
        "  # reshaping the data to 3D to be accepted by our LSTM model\n",
        "  model_input = np.reshape(data, (1, 60, 1))\n",
        "\n",
        "  # loading the model and predicting\n",
        "  loaded_model = load_model(\"lstm_f_60.hdf5\")\n",
        "  model_prediction = float(loaded_model.predict(model_input))\n",
        "  real_prediction = model_prediction + df['Close'][-1]\n",
        "  \n",
        "\n",
        "  return model_prediction, real_prediction\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "@app.get('/')\n",
        "\n",
        "def index():\n",
        "    return {'message': 'Hello!'}\n",
        "\n",
        "@app.post('/predict')\n",
        "\n",
        "async def predict_price(ticker:str):\n",
        "    if data == 'F':\n",
        "      \n",
        "      \n",
        "      model_prediction, lstm_prediction = lstm(data)\n",
        "      \n",
        "  \n",
        "     return {'LSTM prediction' : lstm_prediction}\n",
        "  \n",
        "    else:\n",
        "        return {\"the ticker not supported yet\"}\n",
        "        \n",
        "      \n",
        "       \n",
        "if __name__ == '__main__':\n",
        "    uvicorn.run(app, host='127.0.0.1', port=8000)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acQeKRAXk5D5"
      },
      "source": [
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71yBRE10d2sA"
      },
      "source": [
        "# Models used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEZYoTu7fMB_"
      },
      "source": [
        "def prophet (ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using prophet ! by Getting the desired data from yahoo, then doing some data manipulation, then the comes the prophet's turn\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) prophet_output - the model out-put (the prediction of the next day)\n",
        "  \"\"\"\n",
        "\n",
        "  # data_gathering\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start='2015-01-01')\n",
        "\n",
        "  # data manipulation\n",
        "  holiday = pd.DataFrame([])\n",
        "  for date, name in sorted(holidays.UnitedStates(years=[2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]).items()):\n",
        "      holiday = holiday.append(pd.DataFrame({'ds': date, 'holiday': \"US-Holidays\"}, index=[0]), ignore_index=True)\n",
        "  holiday['ds'] = pd.to_datetime(holiday['ds'], format='%Y-%m-%d', errors='ignore')\n",
        "\n",
        "  # data frame modification to be accepted by prophet\n",
        "  data = df['Close'].reset_index()\n",
        "  data.columns = ['ds', 'y']\n",
        "\n",
        "  # model building\n",
        "  m = Prophet(holidays=holiday,seasonality_mode='additive', changepoint_prior_scale = 0.1, seasonality_prior_scale=0.01)\n",
        "  m.fit(data)\n",
        "\n",
        "  # model predictions\n",
        "  future = m.make_future_dataframe(periods=1)\n",
        "  model_prediction = m.predict(future) \n",
        "  prophet_prediction = float(model_prediction[ 'yhat'][-1:])\n",
        "  return prophet_prediction\n",
        "\n",
        "\n",
        "def arima(ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using ARIMA ! by Getting the desired data from yahoo, \n",
        "  then finding the best order of arima params then the comes the ARIMA's turn\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) arima_output - the model out-put (the prediction of the next day)\n",
        "      (float) diff - the model output - today's price (the diff between tomorrow's prediction and today's real value)\n",
        "  \"\"\"\n",
        "    \n",
        "  # data gathering\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start='2016-01-01')\n",
        "  df.index = pd.to_datetime(df.index, format=\"%Y/%m/%d\")\n",
        "  df = pd.Series(df['Close'])\n",
        "  last_day=df[-1]\n",
        "\n",
        "  # finding the best order\n",
        "  auto_order = pm.auto_arima(df, start_p=0, start_q=0, test='adf', max_p=3, max_q=3, m=1,d=None,seasonal=False   \n",
        "                    ,start_P=0,D=0, trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)\n",
        "  best_order = auto_order.order\n",
        "\n",
        "  # model fitting\n",
        "  model = ARIMA(df, order=best_order)\n",
        "  model_fit = model.fit(disp=0)\n",
        "  arima_prediction ,se, conf = model_fit.forecast(1)\n",
        "  \n",
        "  diff = arima_prediction - last_day\n",
        "  \n",
        "  return arima_prediction , diff\n",
        "\n",
        "\n",
        "def lstm(data_set):\n",
        "  \"\"\"\n",
        "  Getting the desired data from yahoo, then doing some data manipulation such as data\n",
        "  reshaping\n",
        "  Args:\n",
        "      (str) data_set - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) diff_prediction - the model out-put (the prediction of the next day)\n",
        "      (float) real_prediction - the model output + today's price (real price of tomorrow)\n",
        "  \"\"\"\n",
        "\n",
        "  # data gathering\n",
        "  df = pdr.DataReader(data_set, data_source='yahoo', start=date.today() - timedelta(100))\n",
        "\n",
        "  # data manipulation\n",
        "\n",
        "  # creating a new df with Xt - Xt-1 values of the close prices (most recent 60 days)\n",
        "  close_df = df['2012-01-01':].reset_index()['Close'][-61:]\n",
        "  close_diff = close_df.diff().dropna()\n",
        "  data = np.array(close_diff).reshape(-1, 1)\n",
        "\n",
        "  # reshaping the data to 3D to be accepted by our LSTM model\n",
        "  model_input = np.reshape(data, (1, 60, 1))\n",
        "\n",
        "  # loading the model and predicting\n",
        "  loaded_model = load_model(\"lstm_f_60.hdf5\")\n",
        "  model_prediction = float(loaded_model.predict(model_input))\n",
        "  real_prediction = model_prediction + df['Close'][-1]\n",
        "  \n",
        "\n",
        "  return model_prediction, real_prediction\n",
        "\n",
        "\n",
        "def Regression(ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using an ensambled model between SVR, Ridge and Linear regression! by Getting the desired data from yahoo, \n",
        "  then doing some data manipulation\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) arima_output - the model out-put (the prediction of the next day)\n",
        "      (float) diff - the model output - today's price (the diff between tomorrow's prediction and today's real value)\n",
        "  \"\"\"\n",
        "  start_date = datetime.now() - timedelta(1)\n",
        "  start_date = datetime.strftime(start_date, '%Y-%m-%d')\n",
        "\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start=start_date)  # read data\n",
        "  df.drop('Volume', axis='columns', inplace=True)\n",
        "  X = df[['High', 'Low', 'Open', 'Adj Close']]  # input columns\n",
        "  y = df[['Close']]  # output column\n",
        "  input = X\n",
        "  loaded_model = pickle.load(open('regression_model.pkl', 'rb'))\n",
        "  reg_prediction = loaded_model.predict(input)\n",
        "  reg_diff=reg_prediction-df.Close[-1]\n",
        "\n",
        "  return  reg_prediction,reg_diff\n",
        "\n",
        "def Transformer(ticker):\n",
        "  seq_len = 32\n",
        "\n",
        "  start_date = datetime.now() - timedelta(48)\n",
        "  start_date = datetime.strftime(start_date, '%Y-%m-%d')\n",
        "\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start=start_date)\n",
        "\n",
        "  df.drop('Volume', axis=1, inplace=True)\n",
        "\n",
        "  # df[df.columns] = scaler.fit_transform(df)\n",
        "  df = df[['High', 'Low', 'Open', 'Adj Close', 'Close']]\n",
        "\n",
        "  '''Create training, validation and test split'''\n",
        "\n",
        "  test_data = df.values\n",
        "\n",
        "  # Test data\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(seq_len, len(test_data)):\n",
        "      X_test.append(test_data[i - seq_len:i])\n",
        "      y_test.append(test_data[:, 4][i])\n",
        "  X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "  custom_objects = {\"Time2Vector\": Time2Vector,\n",
        "                    \"MultiAttention\": MultiAttention,\n",
        "                    'TransformerEncoder': TransformerEncoder}\n",
        "  with keras.utils.custom_object_scope(custom_objects):\n",
        "      final_model = load_model('Transformer+TimeEmbedding.hdf5')\n",
        "\n",
        "  trans_prediction = float(final_model.predict(X_test)[-1])\n",
        "  trans_difference = trans_prediction - df.Close[-1]\n",
        "\n",
        "  return trans_prediction, trans_difference\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCJq_nLKd7fV"
      },
      "source": [
        "# The APP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCXemwdqdv11",
        "outputId": "597e0842-1093-46f8-cb66-1b6a32e0ae08"
      },
      "source": [
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "    return {'message': 'This is your fav stock predictor!'}\n",
        "\n",
        "\n",
        "@app.post('/predict')\n",
        "async def predict_price(data: str):\n",
        "    if data == 'F':\n",
        "      prophet_prediction = float(prophet(data))\n",
        "      arima_prediction, diff = arima(data)\n",
        "      model_prediction, lstm_prediction = lstm(data)\n",
        "      reg_prediction,reg_diff = Regression(data)\n",
        "      trans_prediction, trans_difference = Transformer(data)\n",
        "\n",
        "      return {\n",
        "        'Prophet prediction': prophet_prediction,\n",
        "        'Arima prediction' : arima_prediction[0],\n",
        "        'LSTM prediction' : lstm_prediction,\n",
        "        'regression prediction' : reg_prediction[0],\n",
        "        'Transformer prediction': trans_prediction\n",
        "            }\n",
        "\n",
        "    else:\n",
        "      return {\"the ticker not supported yet\"}\n",
        "\n",
        "    \n",
        "\n",
        "# App startup\n",
        "ngrok_tunnel = ngrok.connect(8030)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8030)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8030-bd53a612-94a1-438a-bbd0-878048677f00\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:58+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:58+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:58+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:58+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=\"client session established\" obj=csess id=10eed156dadc\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=start pg=/api/tunnels id=09a36813310f7a7e\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=end pg=/api/tunnels id=09a36813310f7a7e status=200 dur=460.242µs\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=start pg=/api/tunnels id=bd6092975d0a06e6\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=end pg=/api/tunnels id=bd6092975d0a06e6 status=200 dur=127.33µs\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=start pg=/api/tunnels id=b608c6d0962e48b6\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8030-bd53a612-94a1-438a-bbd0-878048677f00 (http)\" addr=http://localhost:8030 url=http://386a85a71994.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8030-bd53a612-94a1-438a-bbd0-878048677f00 addr=http://localhost:8030 url=https://386a85a71994.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=end pg=/api/tunnels id=b608c6d0962e48b6 status=201 dur=72.537896ms\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Public URL: http://386a85a71994.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=start pg=\"/api/tunnels/http-8030-bd53a612-94a1-438a-bbd0-878048677f00 (http)\" id=5f785c3dc9571143\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:39:59+0000 lvl=info msg=end pg=\"/api/tunnels/http-8030-bd53a612-94a1-438a-bbd0-878048677f00 (http)\" id=5f785c3dc9571143 status=200 dur=142.248µs\n",
            "INFO:     Started server process [57]\n",
            "INFO:uvicorn.error:Started server process [57]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8030 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8030 (Press CTRL+C to quit)\n",
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:40:05+0000 lvl=info msg=\"join connections\" obj=join id=1eab6cec8df1 l=127.0.0.1:8030 r=154.181.131.137:58672\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     154.181.131.137:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     154.181.131.137:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     154.181.131.137:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     154.181.131.137:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2021-05-12T19:40:18+0000 lvl=info msg=\"join connections\" obj=join id=f0e537b5ce5f l=127.0.0.1:8030 r=154.181.131.137:58672\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Performing stepwise search to minimize aic\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-592.186, Time=0.18 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-590.386, Time=0.11 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-590.387, Time=0.35 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=-594.048, Time=0.08 sec\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=-588.433, Time=0.38 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,0)(0,0,0)[0]          \n",
            "Total fit time: 1.121 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/arima_model.py:472: FutureWarning:\n",
            "\n",
            "\n",
            "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
            "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
            "between arima and model) and\n",
            "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
            "\n",
            "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
            "is both well tested and maintained.\n",
            "\n",
            "To silence this warning and continue using ARMA and ARIMA until they are\n",
            "removed, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
            "                        FutureWarning)\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
            "                        FutureWarning)\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/base/tsa_model.py:583: ValueWarning:\n",
            "\n",
            "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/base/tsa_model.py:583: ValueWarning:\n",
            "\n",
            "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     154.181.131.137:0 - \"POST /predict?data=F HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "755weNtpEvWD"
      },
      "source": [
        "import tensorflow as ts\n",
        "print('py'+str(sys.version))\n",
        "print('tens vers'+ts.__version__)\n",
        "print('keras vers'+keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}