{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models__live.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WThzQN33cTdm"
      },
      "source": [
        "# Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIJnVx6PeSHE"
      },
      "source": [
        "# !pip install uvicorn\n",
        "# !pip install fastapi\n",
        "# !pip install nest_asyncio\n",
        "# !pip install pystan\n",
        "# !pip install prophet\n",
        "# !pip install pyngrok \n",
        "# !pip install pmdarima\n",
        "# !pip install -v scikit-learn==0.23.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IzDl7qxcqYC"
      },
      "source": [
        "# Importations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmOX3UNbh5at"
      },
      "source": [
        "# for FastAPI\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import pydantic\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "# for FBprophet\n",
        "from datetime import *\n",
        "import pandas_datareader as pdr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import holidays\n",
        "from prophet import Prophet\n",
        "# for arima\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import pmdarima as pm\n",
        "# for LSTM\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "# for transformer\n",
        "from utils.Time2Vector import Time2Vector\n",
        "from utils.Attention import MultiAttention, SingleAttention\n",
        "from utils.Encoder import TransformerEncoder\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acQeKRAXk5D5"
      },
      "source": [
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71yBRE10d2sA"
      },
      "source": [
        "# Models used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEZYoTu7fMB_"
      },
      "source": [
        "def prophet (ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using prophet ! by Getting the desired data from yahoo, then doing some data manipulation, then the comes the prophet's turn\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) prophet_output - the model out-put (the prediction of the next day)\n",
        "  \"\"\"\n",
        "\n",
        "  # data_gathering\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start='2015-01-01')\n",
        "\n",
        "  # data manipulation\n",
        "  holiday = pd.DataFrame([])\n",
        "  for date, name in sorted(holidays.UnitedStates(years=[2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]).items()):\n",
        "      holiday = holiday.append(pd.DataFrame({'ds': date, 'holiday': \"US-Holidays\"}, index=[0]), ignore_index=True)\n",
        "  holiday['ds'] = pd.to_datetime(holiday['ds'], format='%Y-%m-%d', errors='ignore')\n",
        "\n",
        "  # data frame modification to be accepted by prophet\n",
        "  data = df['Close'].reset_index()\n",
        "  data.columns = ['ds', 'y']\n",
        "\n",
        "  # model building\n",
        "  m = Prophet(holidays=holiday,seasonality_mode='additive', changepoint_prior_scale = 0.1, seasonality_prior_scale=0.01)\n",
        "  m.fit(data)\n",
        "\n",
        "  # model predictions\n",
        "  future = m.make_future_dataframe(periods=1)\n",
        "  model_prediction = m.predict(future) \n",
        "  prophet_prediction = float(model_prediction[ 'yhat'][-1:])\n",
        "  return prophet_prediction\n",
        "\n",
        "\n",
        "def arima(ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using ARIMA ! by Getting the desired data from yahoo, \n",
        "  then finding the best order of arima params then the comes the ARIMA's turn\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) arima_output - the model out-put (the prediction of the next day)\n",
        "      (float) diff - the model output - today's price (the diff between tomorrow's prediction and today's real value)\n",
        "  \"\"\"\n",
        "    \n",
        "  # data gathering\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start='2016-01-01')\n",
        "  df.index = pd.to_datetime(df.index, format=\"%Y/%m/%d\")\n",
        "  df = pd.Series(df['Close'])\n",
        "  last_day=df[-1]\n",
        "\n",
        "  # finding the best order\n",
        "  auto_order = pm.auto_arima(df, start_p=0, start_q=0, test='adf', max_p=3, max_q=3, m=1,d=None,seasonal=False   \n",
        "                    ,start_P=0,D=0, trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)\n",
        "  best_order = auto_order.order\n",
        "\n",
        "  # model fitting\n",
        "  model = ARIMA(df, order=best_order)\n",
        "  model_fit = model.fit(disp=0)\n",
        "  arima_prediction ,se, conf = model_fit.forecast(1)\n",
        "  \n",
        "  diff = arima_prediction - last_day\n",
        "  \n",
        "  return arima_prediction , diff\n",
        "\n",
        "\n",
        "def lstm(data_set):\n",
        "  \"\"\"\n",
        "  Getting the desired data from yahoo, then doing some data manipulation such as data\n",
        "  reshaping\n",
        "  Args:\n",
        "      (str) data_set - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) diff_prediction - the model out-put (the prediction of the next day)\n",
        "      (float) real_prediction - the model output + today's price (real price of tomorrow)\n",
        "  \"\"\"\n",
        "\n",
        "  # data gathering\n",
        "  df = pdr.DataReader(data_set, data_source='yahoo', start=date.today() - timedelta(100))\n",
        "\n",
        "  # data manipulation\n",
        "\n",
        "  # creating a new df with Xt - Xt-1 values of the close prices (most recent 60 days)\n",
        "  close_df = df['2012-01-01':].reset_index()['Close'][-61:]\n",
        "  close_diff = close_df.diff().dropna()\n",
        "  data = np.array(close_diff).reshape(-1, 1)\n",
        "\n",
        "  # reshaping the data to 3D to be accepted by our LSTM model\n",
        "  model_input = np.reshape(data, (1, 60, 1))\n",
        "\n",
        "  # loading the model and predicting\n",
        "  loaded_model = load_model(\"lstm_f_60.hdf5\")\n",
        "  model_prediction = float(loaded_model.predict(model_input))\n",
        "  real_prediction = model_prediction + df['Close'][-1]\n",
        "  \n",
        "\n",
        "  return model_prediction, real_prediction\n",
        "\n",
        "\n",
        "def Regression(ticker):\n",
        "  \"\"\"\n",
        "  Forcasting using an ensambled model between SVR, Ridge and Linear regression! by Getting the desired data from yahoo, \n",
        "  then doing some data manipulation\n",
        "  Args:\n",
        "      (str) ticket - the ticker of desired dataset (company)\n",
        "  Returns:\n",
        "      (float) arima_output - the model out-put (the prediction of the next day)\n",
        "      (float) diff - the model output - today's price (the diff between tomorrow's prediction and today's real value)\n",
        "  \"\"\"\n",
        "  start_date = datetime.now() - timedelta(1)\n",
        "  start_date = datetime.strftime(start_date, '%Y-%m-%d')\n",
        "\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start=start_date)  # read data\n",
        "  df.drop('Volume', axis='columns', inplace=True)\n",
        "  X = df[['High', 'Low', 'Open', 'Adj Close']]  # input columns\n",
        "  y = df[['Close']]  # output column\n",
        "  input = X\n",
        "  loaded_model = pickle.load(open('regression_model.pkl', 'rb'))\n",
        "  reg_prediction = loaded_model.predict(input)\n",
        "  reg_diff=reg_prediction-df.Close[-1]\n",
        "\n",
        "  return  reg_prediction,reg_diff\n",
        "\n",
        "def Transformer(ticker):\n",
        "  seq_len = 32\n",
        "\n",
        "  start_date = datetime.now() - timedelta(48)\n",
        "  start_date = datetime.strftime(start_date, '%Y-%m-%d')\n",
        "\n",
        "  df = pdr.DataReader(ticker, data_source='yahoo', start=start_date)\n",
        "\n",
        "  df.drop('Volume', axis=1, inplace=True)\n",
        "\n",
        "  # df[df.columns] = scaler.fit_transform(df)\n",
        "  df = df[['High', 'Low', 'Open', 'Adj Close', 'Close']]\n",
        "\n",
        "  '''Create training, validation and test split'''\n",
        "\n",
        "  test_data = df.values\n",
        "\n",
        "  # Test data\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(seq_len, len(test_data)):\n",
        "      X_test.append(test_data[i - seq_len:i])\n",
        "      y_test.append(test_data[:, 4][i])\n",
        "  X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "  custom_objects = {\"Time2Vector\": Time2Vector,\n",
        "                    \"MultiAttention\": MultiAttention,\n",
        "                    'TransformerEncoder': TransformerEncoder}\n",
        "  with keras.utils.custom_object_scope(custom_objects):\n",
        "      final_model = load_model('Transformer+TimeEmbedding.hdf5')\n",
        "\n",
        "  trans_prediction = float(final_model.predict(X_test)[-1])\n",
        "  trans_difference = trans_prediction - df.Close[-1]\n",
        "\n",
        "  return trans_prediction, trans_difference\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCJq_nLKd7fV"
      },
      "source": [
        "# The APP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCXemwdqdv11"
      },
      "source": [
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "    return {'message': 'This is your fav stock predictor!'}\n",
        "\n",
        "\n",
        "@app.post('/predict')\n",
        "async def predict_price(data: str):\n",
        "    if data == 'F':\n",
        "      prophet_prediction = float(prophet(data))\n",
        "      arima_prediction, diff = arima(data)\n",
        "      model_prediction, lstm_prediction = lstm(data)\n",
        "      reg_prediction,reg_diff = Regression(data)\n",
        "      trans_prediction, trans_difference = Transformer(data)\n",
        "\n",
        "      return {\n",
        "        'Prophet prediction': prophet_prediction,\n",
        "        'Arima prediction' : arima_prediction[0],\n",
        "        'LSTM prediction' : lstm_prediction,\n",
        "        'regression prediction' : reg_prediction[0],\n",
        "        'Transformer prediction': trans_prediction\n",
        "            }\n",
        "\n",
        "    else:\n",
        "      return {\"the ticker not supported yet\"}\n",
        "\n",
        "    \n",
        "\n",
        "# App startup\n",
        "ngrok_tunnel = ngrok.connect(8030)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8030)"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}